{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\buzlab\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\utils\\generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1fb8d61f170>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from tqdm import tqdm\n",
    "from peft import PeftModel\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from data.data_utils import *\n",
    "\n",
    "from pp_utils import (\n",
    "    compute_topk_components,\n",
    "    eval_circuit_performance,\n",
    "    get_mean_activations,\n",
    "    get_random_circuit,\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "seed = 5\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5afad46a1640d4b95dd554b6c06cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93b5070d0c048b2a39420383be23124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17b6f13176fa4e7296aed1b509291918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7cb5d6a5a64d7bbb4080b68aa4e1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"hf-internal-testing/llama-tokenizer\", padding_side=\"right\"\n",
    ")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 500\n"
     ]
    }
   ],
   "source": [
    "data_file = \"../data/dataset.jsonl\"\n",
    "batch_size = 50\n",
    "\n",
    "raw_data = sample_box_data(\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=500,\n",
    "    data_file=data_file,\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_data[0],\n",
    "        \"last_token_indices\": raw_data[1],\n",
    "        \"labels\": raw_data[2],\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./results/circuits/llama_circuit.json\", \"r\") as f:\n",
    "    llama_circuit = json.load(f)\n",
    "\n",
    "with open(\"./results/circuits/goat_circuit.json\", \"r\") as f:\n",
    "    goat_circuit = json.load(f)\n",
    "\n",
    "with open(\n",
    "    \"./results/circuits/float_circuit.json\", \"r\") as f:\n",
    "    float_circuit = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama Circuit\n",
      "Value Fetcher Heads: 40\n",
      "Position Transmitter Heads: 7\n",
      "Position Detector Heads: 20\n",
      "Structure Reader Heads: 5\n",
      "Total Heads: 72\n"
     ]
    }
   ],
   "source": [
    "print(\"Llama Circuit\")\n",
    "print(f\"Value Fetcher Heads: {len(llama_circuit['value_fetcher'])}\")\n",
    "print(f\"Position Transmitter Heads: {len(llama_circuit['pos_transmitter'])}\")\n",
    "print(f\"Position Detector Heads: {len(llama_circuit['pos_detector'])}\")\n",
    "print(f\"Structure Reader Heads: {len(llama_circuit['struct_reader'])}\")\n",
    "print(\n",
    "    f\"Total Heads: {len(llama_circuit['value_fetcher']) + len(llama_circuit['pos_transmitter']) + len(llama_circuit['pos_detector']) + len(llama_circuit['struct_reader'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOAT CIRCUIT\n",
      "Value Fetcher Heads: 68\n",
      "Position Transmitter Heads: 28\n",
      "Position Detector Heads: 40\n",
      "Structure Reader Heads: 39\n",
      "Total Heads: 175\n"
     ]
    }
   ],
   "source": [
    "print(\"GOAT CIRCUIT\")\n",
    "print(f\"Value Fetcher Heads: {len(goat_circuit['value_fetcher'])}\")\n",
    "print(f\"Position Transmitter Heads: {len(goat_circuit['pos_transmitter'])}\")\n",
    "print(f\"Position Detector Heads: {len(goat_circuit['pos_detector'])}\")\n",
    "print(f\"Structure Reader Heads: {len(goat_circuit['struct_reader'])}\")\n",
    "print(\n",
    "    f\"Total Heads: {len(goat_circuit['value_fetcher']) + len(goat_circuit['pos_transmitter']) + len(goat_circuit['pos_detector']) + len(goat_circuit['struct_reader'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float Circuit\n",
      "Value Fetcher Heads: 68\n",
      "Position Transmitter Heads: 29\n",
      "Position Detector Heads: 40\n",
      "Structure Reader Heads: 38\n",
      "Total Heads: 175\n"
     ]
    }
   ],
   "source": [
    "print(\"Float Circuit\")\n",
    "print(f\"Value Fetcher Heads: {len(float_circuit['value_fetcher'])}\")\n",
    "print(f\"Position Transmitter Heads: {len(float_circuit['pos_transmitter'])}\")\n",
    "print(f\"Position Detector Heads: {len(float_circuit['pos_detector'])}\")\n",
    "print(f\"Structure Reader Heads: {len(float_circuit['struct_reader'])}\")\n",
    "print(\n",
    "    f\"Total Heads: {len(float_circuit['value_fetcher']) + len(float_circuit['pos_transmitter']) + len(float_circuit['pos_detector']) + len(float_circuit['struct_reader'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_circuit(model, circuit_heads):\n",
    "    circuit_components = {}\n",
    "    circuit_components[0] = defaultdict(list)\n",
    "    circuit_components[2] = defaultdict(list)\n",
    "    circuit_components[-1] = defaultdict(list)\n",
    "\n",
    "    for layer_idx, head in circuit_heads[\"value_fetcher\"]:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[0][layer].append(head)\n",
    "\n",
    "    for layer_idx, head in circuit_heads[\"pos_transmitter\"]:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[0][layer].append(head)\n",
    "\n",
    "    for layer_idx, head in circuit_heads[\"pos_detector\"]:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[2][layer].append(head)\n",
    "\n",
    "    for layer_idx, head in circuit_heads[\"struct_reader\"]:\n",
    "        if model.config.architectures[0] == \"LlamaForCausalLM\":\n",
    "            layer = f\"model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        else:\n",
    "            layer = f\"base_model.model.model.layers.{layer_idx}.self_attn.o_proj\"\n",
    "        circuit_components[-1][layer].append(head)\n",
    "\n",
    "    return circuit_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_performance(model, dataloader):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for _, output in tqdm(enumerate(tqdm(dataloader))):\n",
    "            for k, v in output.items():\n",
    "                if v is not None and isinstance(v, torch.Tensor):\n",
    "                    output[k] = v.to(model.device)\n",
    "\n",
    "            outputs = model(input_ids=output[\"input_ids\"])\n",
    "\n",
    "            for bi in range(output[\"labels\"].size(0)):\n",
    "                label = output[\"labels\"][bi]\n",
    "                pred = torch.argmax(\n",
    "                    outputs.logits[bi][output[\"last_token_indices\"][bi]]\n",
    "                )\n",
    "\n",
    "                if label == pred:\n",
    "                    correct_count += 1\n",
    "                total_count += 1\n",
    "\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    current_acc = round(correct_count / total_count, 2)\n",
    "    return current_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Llama-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee040c20d03c415cb019554cdec689fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"/home/local_nikhil/Projects/llama_weights/7B\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]100%|██████████| 10/10 [00:21<00:00,  2.15s/it]\n"
     ]
    }
   ],
   "source": [
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.15s/it]\n",
      "10it [00:21,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance 0.66\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit Performance 0.66\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.55s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.55s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.57s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.50s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.51s/it]\n",
      "100%|██████████| 10/10 [00:44<00:00,  4.50s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.54s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.53s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Circuit Performance 0.0\n",
      "Faithfulness: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\\n\")\n",
    "\n",
    "circuit_components = get_circuit(model, llama_circuit)\n",
    "circuit_acc = eval_circuit_performance(\n",
    "    model, dataloader, modules, circuit_components, mean_activations\n",
    ")\n",
    "print(f\"Circuit Performance {circuit_acc}\\n\")\n",
    "\n",
    "random_circuit_acc = 0\n",
    "n_iters = 10\n",
    "for _ in range(n_iters):\n",
    "    random_circuit_components = get_random_circuit(model, llama_circuit)\n",
    "    random_circuit_acc += eval_circuit_performance(\n",
    "        model, dataloader, modules, random_circuit_components, mean_activations\n",
    "    )\n",
    "random_circuit_acc = round(random_circuit_acc / n_iters, 2)\n",
    "print(f\"Random Circuit Performance {random_circuit_acc}\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Vicuna-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a6a1bcb1894c79ac57966f06021561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d73b4272dc4b499ba34c974343f45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3915bdaf253641e5a7adae5be607614b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6da762325a74e03903c35310917b342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00014.bin:   0%|          | 0.00/981M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626b1f4b2c9b477da11f3c7c7e017026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6979c974e646ffb9af015d67be5981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a9adb5ed7f43ba907ba2072d19fa7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00004-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ba865740b80467399d79e2e09491ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00005-of-00014.bin:   0%|          | 0.00/944M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2157a6bcd0e54db1a50ed2638e94e1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00006-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b60c68d0434cde8be742bed3e05bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00007-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7c15cc2a7f3472485d7a2616ad2be1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00008-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94aa88a9f6c3482e9033a0ccff15c950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00009-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4afa2b1f3dd9400b87cc46d949dacf28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00010-of-00014.bin:   0%|          | 0.00/944M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc7593d2b15450087d9fb08aa877bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00011-of-00014.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962d179c176e478fbacfa3b43530a64a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00012-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ef4c253bec4fc28c6b2a39df4d1660",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00013-of-00014.bin:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5fda5d938424b3ca8aff0a645f1a72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00014-of-00014.bin:   0%|          | 0.00/847M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d51b90fe074754a7914df32d4ee057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59fc5e29f3944208eef3ad5cb0f4367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"AlekseyKorshuk/vicuna-7b\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.14s/it]\n"
     ]
    }
   ],
   "source": [
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:21<00:00,  2.14s/it]\n",
      "10it [00:21,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance 0.67\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit Performance 0.65\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.55s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.55s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.53s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.51s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.53s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.54s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.53s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.54s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.54s/it]\n",
      "100%|██████████| 10/10 [00:45<00:00,  4.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Circuit Performance 0.0\n",
      "\n",
      "Faithfulness: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\\n\")\n",
    "\n",
    "circuit_components = get_circuit(model, llama_circuit)\n",
    "circuit_acc = eval_circuit_performance(\n",
    "    model, dataloader, modules, circuit_components, mean_activations\n",
    ")\n",
    "print(f\"Circuit Performance {circuit_acc}\\n\")\n",
    "\n",
    "random_circuit_acc = 0\n",
    "n_iters = 10\n",
    "for _ in range(n_iters):\n",
    "    random_circuit_components = get_random_circuit(model, llama_circuit)\n",
    "    random_circuit_acc += eval_circuit_performance(\n",
    "        model, dataloader, modules, random_circuit_components, mean_activations\n",
    "    )\n",
    "random_circuit_acc = round(random_circuit_acc / n_iters, 2)\n",
    "print(f\"Random Circuit Performance {random_circuit_acc}\\n\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Goat-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "decapoda-research/llama-7b-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:261\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 261\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/decapoda-research/llama-7b-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\utils\\hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\huggingface_hub\\file_download.py:1346\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1348\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\huggingface_hub\\file_download.py:1232\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1232\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\huggingface_hub\\file_download.py:1608\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1599\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1600\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1601\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1606\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   1607\u001b[0m )\n\u001b[1;32m-> 1608\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:293\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    285\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    292\u001b[0m     )\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-664bcc91-230f6b1228d77abf5de2cb53;db89ba57-4e2d-47c4-82d9-ebcf549bcb34)\n\nRepository Not Found for url: https://huggingface.co/decapoda-research/llama-7b-hf/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m     16\u001b[0m     model,\n\u001b[0;32m     17\u001b[0m     lora_weights,\n\u001b[0;32m     18\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m     19\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m},\n\u001b[0;32m     20\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\modeling_utils.py:2600\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   2599\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[1;32m-> 2600\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2605\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2606\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2607\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2608\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2609\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2610\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2611\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2612\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2613\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2614\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m   2615\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\finetuning\\lib\\site-packages\\transformers\\utils\\hub.py:451\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 451\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    454\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    456\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    459\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: decapoda-research/llama-7b-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "base_model = \"decapoda-research/llama-7b-hf\"\n",
    "lora_weights = \"tiedong/goat-lora-7b\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    load_in_8bit=False,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    lora_weights,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map={\"\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.23s/it]\n",
      "10it [00:22,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance: 0.82\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:45<00:00,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circuit Performance: 0.73\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:46<00:00,  4.62s/it]\n",
      "100%|██████████| 10/10 [00:46<00:00,  4.63s/it]\n",
      "100%|██████████| 10/10 [00:46<00:00,  4.62s/it]\n",
      "100%|██████████| 10/10 [00:46<00:00,  4.69s/it]\n",
      "100%|██████████| 10/10 [00:46<00:00,  4.70s/it]\n",
      "100%|██████████| 10/10 [00:46<00:00,  4.69s/it]\n",
      "100%|██████████| 10/10 [00:46<00:00,  4.69s/it]\n",
      "100%|██████████| 10/10 [00:46<00:00,  4.67s/it]\n",
      "100%|██████████| 10/10 [00:46<00:00,  4.70s/it]\n",
      "100%|██████████| 10/10 [00:46<00:00,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Circuit Performance 0.01\n",
      "\n",
      "Faithfulness: 0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance: {model_acc}\\n\")\n",
    "\n",
    "circuit_components = get_circuit(model, llama_circuit)\n",
    "circuit_acc = eval_circuit_performance(\n",
    "    model, dataloader, modules, circuit_components, mean_activations\n",
    ")\n",
    "print(f\"Circuit Performance: {circuit_acc}\\n\")\n",
    "\n",
    "random_circuit_acc = 0\n",
    "n_iters = 10\n",
    "for _ in range(n_iters):\n",
    "    random_circuit_components = get_random_circuit(model, llama_circuit)\n",
    "    random_circuit_acc += eval_circuit_performance(\n",
    "        model, dataloader, modules, random_circuit_components, mean_activations\n",
    "    )\n",
    "random_circuit_acc = round(random_circuit_acc / n_iters, 2)\n",
    "print(f\"Random Circuit Performance {random_circuit_acc}\\n\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Circuit Performance: Float-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b42175e69c0482ebb23123708471bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef40323a31914b05a4cb57cb1ddcc709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a775be93c6964848b26f59ea634c13cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7876212ec5457f98b3d48b63b4c7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f985aceebf942d086fe2194b94a20c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee486453fc646439e633099fc17022c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4b4103214c41409af33af21036f835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"nikhil07prakash/float-7b\"\n",
    "\n",
    "# Delete model if present in memory\n",
    "if \"model\" in locals():\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mean activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████                                         | 5/10 [09:13<08:59, 107.88s/it]"
     ]
    }
   ],
   "source": [
    "mean_activations, modules = get_mean_activations(\n",
    "    model, tokenizer, data_file, num_samples=500, batch_size=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_acc = eval_model_performance(model, dataloader)\n",
    "print(f\"Model Performance {model_acc}\")\n",
    "\n",
    "circuit_components = get_circuit(model, llama_circuit)\n",
    "circuit_acc = eval_circuit_performance(\n",
    "    model, dataloader, modules, circuit_components, mean_activations\n",
    ")\n",
    "print(f\"Circuit Performance {circuit_acc}\")\n",
    "\n",
    "random_circuit_acc = 0\n",
    "n_iters = 10\n",
    "for _ in range(n_iters):\n",
    "    random_circuit_components = get_random_circuit(model, llama_circuit)\n",
    "    random_circuit_acc += eval_circuit_performance(\n",
    "        model, dataloader, modules, random_circuit_components, mean_activations\n",
    "    )\n",
    "random_circuit_acc = round(random_circuit_acc / n_iters, 2)\n",
    "print(f\"Random Circuit Performance {random_circuit_acc}\\n\")\n",
    "\n",
    "print(f\"Faithfulness: {round(circuit_acc/model_acc, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_groups = [\"value_fetcher\", \"pos_transmitter\", \"pos_detector\", \"struct_reader\"]\n",
    "\n",
    "for head_group in head_groups:\n",
    "    intersection = 0\n",
    "    for l, h in llama_circuit[head_group]:\n",
    "        if [l, h] in float_circuit[head_group]:\n",
    "            intersection += 1\n",
    "\n",
    "    precision = round(intersection / len(llama_circuit[head_group]), 2)\n",
    "    recall = round(intersection / len(float_circuit[head_group]), 2)\n",
    "\n",
    "    print(\n",
    "        f\"{head_group}: {len(llama_circuit[head_group])} | {len(float_circuit[head_group])} | {intersection} | {precision} | {recall}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_value_fetcher = 58\n",
    "n_pos_trans = 10\n",
    "n_pos_detect = 25\n",
    "n_struct_read = 5\n",
    "pp_llama_root = \"./results/path_patching/llama_circuit\"\n",
    "\n",
    "path = f\"{pp_llama_root}/value_fetcher.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_value_fetcher_llama_heads, top_value_fetcher_llama_score = compute_topk_components(\n",
    "    patching_scores, k=n_value_fetcher, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_llama_root}/pos_transmitter.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "(\n",
    "    top_pos_transmitter_llama_heads,\n",
    "    top_pos_transmitter_llama_score,\n",
    ") = compute_topk_components(\n",
    "    patching_scores, k=n_pos_trans, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_llama_root}/pos_detector.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_pos_detector_llama_heads, top_pos_detector_llama_score = compute_topk_components(\n",
    "    patching_scores, k=n_pos_detect, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_llama_root}/struct_reader.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_struct_reader_llama_heads, top_struct_reader_llama_score = compute_topk_components(\n",
    "    patching_scores, k=n_struct_read, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "# If a head is present in both top value_fetcher and position_transmitter, then\n",
    "# remove it from the top value_fetcher heads.\n",
    "intersection = []\n",
    "for head in top_value_fetcher_llama_heads:\n",
    "    if head in top_pos_transmitter_llama_heads:\n",
    "        intersection.append(head)\n",
    "\n",
    "for head in intersection:\n",
    "    idx = top_value_fetcher_llama_heads.index(head)\n",
    "    top_value_fetcher_llama_heads.pop(idx)\n",
    "    top_value_fetcher_llama_score.pop(idx)\n",
    "\n",
    "print(f\"Value Fetcher: {len(top_value_fetcher_llama_heads)}\")\n",
    "print(f\"Position Transmitter: {len(top_pos_transmitter_llama_heads)}\")\n",
    "print(f\"Position Detector: {len(top_pos_detector_llama_heads)}\")\n",
    "print(f\"Structure Reader: {len(top_struct_reader_llama_heads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_value_fetcher = llama_circuit[\"value_fetcher\"]\n",
    "llama_pos_transmitter = llama_circuit[\"pos_transmitter\"]\n",
    "llama_pos_detector = llama_circuit[\"pos_detector\"]\n",
    "llama_struct_reader = llama_circuit[\"struct_reader\"]\n",
    "\n",
    "print(\n",
    "    len(llama_value_fetcher),\n",
    "    len(llama_pos_transmitter),\n",
    "    len(llama_pos_detector),\n",
    "    len(llama_struct_reader),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuple of (head, patching_score, position) for each group in llama circuit\n",
    "llama_value_fetcher_tuple = []\n",
    "for head, score in zip(top_value_fetcher_llama_heads, top_value_fetcher_llama_score):\n",
    "    llama_value_fetcher_tuple.append((head, abs(score), 0))\n",
    "\n",
    "llama_pos_transmitter_tuple = []\n",
    "for head, score in zip(\n",
    "    top_pos_transmitter_llama_heads, top_pos_transmitter_llama_score\n",
    "):\n",
    "    llama_pos_transmitter_tuple.append((head, abs(score), 0))\n",
    "\n",
    "llama_pos_detector_tuple = []\n",
    "for head, score in zip(top_pos_detector_llama_heads, top_pos_detector_llama_score):\n",
    "    llama_pos_detector_tuple.append((head, abs(score), 2))\n",
    "\n",
    "llama_struct_reader_tuple = []\n",
    "for head, score in zip(top_struct_reader_llama_heads, top_struct_reader_llama_score):\n",
    "    llama_struct_reader_tuple.append((head, abs(score), -1))\n",
    "\n",
    "llama_circuit_tuple = (\n",
    "    llama_value_fetcher_tuple\n",
    "    + llama_pos_transmitter_tuple\n",
    "    + llama_pos_detector_tuple\n",
    "    + llama_struct_reader_tuple\n",
    ")\n",
    "llama_circuit_tuple = sorted(llama_circuit_tuple, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_value_fetcher = 102\n",
    "n_pos_trans = 30\n",
    "n_pos_detect = 50\n",
    "n_struct_read = 40\n",
    "pp_float_root = \"./results/path_patching/float_circuit\"\n",
    "\n",
    "path = f\"{pp_float_root}/value_fetcher.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_value_fetcher_float_heads, top_value_fetcher_float_score = compute_topk_components(\n",
    "    patching_scores, k=n_value_fetcher, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_float_root}/pos_transmitter.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "(\n",
    "    top_pos_transmitter_float_heads,\n",
    "    top_pos_transmitter_float_score,\n",
    ") = compute_topk_components(\n",
    "    patching_scores, k=n_pos_trans, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_float_root}/pos_detector.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_pos_detector_float_heads, top_pos_detector_float_score = compute_topk_components(\n",
    "    patching_scores, k=n_pos_detect, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "path = f\"{pp_float_root}/struct_reader.pt\"\n",
    "patching_scores = torch.load(path)\n",
    "top_struct_reader_float_heads, top_struct_reader_float_score = compute_topk_components(\n",
    "    patching_scores, k=n_struct_read, largest=False, return_values=True\n",
    ")\n",
    "\n",
    "# If a head is present in both top value_fetcher and position_transmitter, then\n",
    "# remove it from the top value_fetcher heads.\n",
    "intersection = []\n",
    "for head in top_value_fetcher_float_heads:\n",
    "    if head in top_pos_transmitter_float_heads:\n",
    "        intersection.append(head)\n",
    "\n",
    "for head in intersection:\n",
    "    idx = top_value_fetcher_float_heads.index(head)\n",
    "    top_value_fetcher_float_heads.pop(idx)\n",
    "    top_value_fetcher_float_score.pop(idx)\n",
    "\n",
    "print(f\"Value Fetcher: {len(top_value_fetcher_float_heads)}\")\n",
    "print(f\"Position Transmitter: {len(top_pos_transmitter_float_heads)}\")\n",
    "print(f\"Position Detector: {len(top_pos_detector_float_heads)}\")\n",
    "print(f\"Structure Reader: {len(top_struct_reader_float_heads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_value_fetcher = float_circuit[\"value_fetcher\"]\n",
    "float_pos_transmitter = float_circuit[\"pos_transmitter\"]\n",
    "float_pos_detector = float_circuit[\"pos_detector\"]\n",
    "float_struct_reader = float_circuit[\"struct_reader\"]\n",
    "\n",
    "print(\n",
    "    len(float_value_fetcher),\n",
    "    len(float_pos_transmitter),\n",
    "    len(float_pos_detector),\n",
    "    len(float_struct_reader),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of tuple of (head, patching_score, position) for each group in Goat circuit\n",
    "float_value_fetcher_tuple = []\n",
    "for head, score in zip(top_value_fetcher_float_heads, top_value_fetcher_float_score):\n",
    "    float_value_fetcher_tuple.append((head, abs(score), 0))\n",
    "\n",
    "float_pos_transmitter_tuple = []\n",
    "for head, score in zip(top_pos_transmitter_float_heads, top_pos_transmitter_float_score):\n",
    "    float_pos_transmitter_tuple.append((head, abs(score), 0))\n",
    "\n",
    "float_pos_detector_tuple = []\n",
    "for head, score in zip(top_pos_detector_float_heads, top_pos_detector_float_score):\n",
    "    float_pos_detector_tuple.append((head, abs(score), 2))\n",
    "\n",
    "float_struct_reader_tuple = []\n",
    "for head, score in zip(top_struct_reader_float_heads, top_struct_reader_float_score):\n",
    "    float_struct_reader_tuple.append((head, abs(score), -1))\n",
    "\n",
    "float_circuit_tuple = (\n",
    "    float_value_fetcher_tuple\n",
    "    + float_pos_transmitter_tuple\n",
    "    + float_pos_detector_tuple\n",
    "    + float_struct_reader_tuple\n",
    ")\n",
    "float_circuit_tuple = sorted(float_circuit_tuple, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each head in float_circuit_tuple, find the corresponding head in llama_circuit_tuple\n",
    "# and append it to a new list. If the head is not present in llama_circuit_tuple, then\n",
    "# append it the same list with a patching score of 0.\n",
    "ordered_llama_circuit_tuple = []\n",
    "for f_head, f_score, f_pos in float_circuit_tuple:\n",
    "    found = False\n",
    "    for l_head, l_score, l_pos in llama_circuit_tuple:\n",
    "        if f_head == l_head and f_pos == l_pos:\n",
    "            ordered_llama_circuit_tuple.append((l_head, l_score, l_pos))\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    if not found:\n",
    "        ordered_llama_circuit_tuple.append((f_head, 0, f_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 5), sharex=True)\n",
    "\n",
    "# Use times new roman font\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams[\"font.size\"] = 18\n",
    "\n",
    "ax1.bar(\n",
    "    [i for i in range(len(float_circuit_tuple))],\n",
    "    [score for _, score, _ in float_circuit_tuple],\n",
    "    color=\"red\",\n",
    ")\n",
    "for i in range(len(float_circuit_tuple)):\n",
    "    if (\n",
    "        float_circuit_tuple[i][0] not in float_value_fetcher\n",
    "        and float_circuit_tuple[i][0] not in float_pos_transmitter\n",
    "        and float_circuit_tuple[i][0] not in float_pos_detector\n",
    "        and float_circuit_tuple[i][0] not in float_struct_reader\n",
    "    ):\n",
    "        ax1.bar(\n",
    "            i,\n",
    "            float_circuit_tuple[i][1],\n",
    "            color=\"silver\",\n",
    "        )\n",
    "\n",
    "red_patch = mpatches.Patch(color=\"red\", label=\"Heads in FLoat Circuit\")\n",
    "silver_patch = mpatches.Patch(color=\"silver\", label=\"Heads NOT in FLoat Circuit\")\n",
    "ax1.legend(handles=[red_patch, silver_patch], loc=\"upper right\")\n",
    "\n",
    "ax2.bar(\n",
    "    [i for i in range(len(ordered_llama_circuit_tuple))],\n",
    "    [score for _, score, _ in ordered_llama_circuit_tuple],\n",
    "    color=\"green\",\n",
    ")\n",
    "for i in range(len(ordered_llama_circuit_tuple)):\n",
    "    if (\n",
    "        ordered_llama_circuit_tuple[i][0] not in llama_value_fetcher\n",
    "        and ordered_llama_circuit_tuple[i][0] not in llama_pos_transmitter\n",
    "        and ordered_llama_circuit_tuple[i][0] not in llama_pos_detector\n",
    "        and ordered_llama_circuit_tuple[i][0] not in llama_struct_reader\n",
    "    ):\n",
    "        ax2.bar(\n",
    "            i,\n",
    "            ordered_llama_circuit_tuple[i][1],\n",
    "            color=\"silver\",\n",
    "        )\n",
    "\n",
    "green_patch = mpatches.Patch(color=\"green\", label=\"Heads in Llama Circuit\")\n",
    "silver_patch = mpatches.Patch(color=\"silver\", label=\"Heads NOT in Llama Circuit\")\n",
    "ax2.legend(handles=[green_patch, silver_patch], loc=\"lower right\")\n",
    "\n",
    "# Remove the blank space before and after the bar chart\n",
    "plt.xlim(-1, 200 - 0.5)\n",
    "\n",
    "# Reduce the gap between the two subplots\n",
    "plt.subplots_adjust(hspace=0.01)\n",
    "# Flip the y-axis of the second subplot\n",
    "ax2.invert_yaxis()\n",
    "# Remove 0.0 from the y-axis of the second subplot\n",
    "ax1.set_yticks([0.1, 0.2, 0.3])\n",
    "ax2.set_yticks([0.1, 0.2, 0.3])\n",
    "\n",
    "# Set a single y-axis label for both subplots\n",
    "fig.text(0.07, 0.5, \"Absolute Patching Score\", va=\"center\", rotation=\"vertical\")\n",
    "\n",
    "ax2.set_xlabel(\"Heads\")\n",
    "ax1.set_title(\"Causal Impact of Heads in Llama and FLoat Circuit\")\n",
    "\n",
    "# Remove plox plot frame\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "ax1.spines[\"right\"].set_visible(False)\n",
    "ax1.spines[\"bottom\"].set_visible(False)\n",
    "ax1.spines[\"left\"].set_visible(False)\n",
    "\n",
    "ax2.spines[\"top\"].set_visible(False)\n",
    "ax2.spines[\"right\"].set_visible(False)\n",
    "ax2.spines[\"bottom\"].set_visible(False)\n",
    "ax2.spines[\"left\"].set_visible(False)\n",
    "\n",
    "# Save the plot\n",
    "# plt.plot()\n",
    "plt.savefig(\"./results/figures/float_llama_causal_impact.pdf\", bbox_inches=\"tight\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
